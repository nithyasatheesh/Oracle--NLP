{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3546ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('EcomReviews_8k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7352e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7750ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer    = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    n=3\n",
    "    sep=' '\n",
    "    text = text.lower() # Lower case\n",
    "    tokens = re.findall(r'\\w+', text) # Extract tokens using regex\n",
    "#     tokens =  word_tokenize(text) # Extract tokens using nltk\n",
    "#     tokens = [ lemmatizer.lemmatize(word) for word in tokens] # Lammatization\n",
    "#     tokens = [stemmer.stem(word) for word in tokens] # Stemming\n",
    "    tokens = [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)]) if len([t for t in ngram if t in stopwords])==0]   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data using the built in cleaner in gensim\n",
    "df['text_clean'] = df['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbba2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the label column\n",
    "df['labels']=df['labels'].replace({'__label__2':1,'__label__1':0})\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_clean'], df['labels'] , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model from scratch\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2,sg=0)#cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a68342",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.index_to_key)\n",
    "## Accessing the index_to_key attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.wv['light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c781e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2v_model.wv.index_to_key )\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming X_train and X_test are lists of sequences of words\n",
    "\n",
    "# Convert words to word vectors and pad sequences\n",
    "X_train_vect = pad_sequences([[w2v_model.wv[i] for i in ls if i in words] for ls in X_train], dtype='float32', padding='post')\n",
    "X_test_vect = pad_sequences([[w2v_model.wv[i] for i in ls if i in words] for ls in X_test], dtype='float32', padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the length of the sentence different than the length of the sentence vector?\n",
    "for i, v in enumerate(X_train_vect):\n",
    "    print(len(X_train.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77574e1e",
   "metadata": {},
   "source": [
    "If you're using word embeddings like Word2Vec or GloVe, each word in the sentence is typically represented by a fixed-length vector. When these vectors are combined to represent a sentence, they might be aggregated in various ways (e.g., averaging, summing, or concatenating), resulting in a single vector representation for the entire sentence. Consequently, the length of the sentence vector will not be the same as the number of words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sentence, vector) in enumerate(zip(X_train, X_train_vect)):\n",
    "    print(\"Original Sentence:\", X_train.iloc[i])\n",
    "    print(\"Sentence Vector:\", vector)\n",
    "    print(\"Lengths:\", len(X_train.iloc[i]), len(vector))\n",
    "#relationship between the original sentences and their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3437842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecac22d",
   "metadata": {},
   "source": [
    "##### computes sentence vectors by averaging the word vectors for the words contained in each sentence. \n",
    "- X_train_vect_avg and X_test_vect_avg are initialized as empty lists to store the averaged sentence vectors for the training and test datasets, respectively.\n",
    "- The code iterates over each sentence vector in X_train_vect and X_test_vect. For each sentence vector v, it checks if v is not empty (i.e., it contains at least one word vector). If v is not empty, it computes the mean (average) of the word vectors along the first axis (axis 0), which corresponds to averaging the word vectors for each dimension. The resulting mean vector represents the averaged sentence vector for the current sentence. If v is empty (i.e., all word vectors are zero vectors), it appends a zero vector of the same dimensionality (100 in this case) to the list as a placeholder.\n",
    "- After iterating over all sentence vectors, X_train_vect_avg and X_test_vect_avg contain the averaged sentence vectors for the training and test datasets, respectively.\n",
    "- This approach of averaging word vectors to obtain sentence vectors is a common technique in natural language processing (NLP) tasks. It allows you to capture the overall semantic meaning of a sentence based on the meanings of its constituent words.\n",
    "\n",
    "- The np.zeros(100, dtype=float) part in the code creates a zero vector of length 100, which matches the dimensionality of the word vectors. This zero vector is used as a placeholder for sentences with no words or out-of-vocabulary words, ensuring that all sentence vectors have the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are our sentence vector lengths consistent?\n",
    "for i, v in enumerate(X_train_vect_avg):\n",
    "    print(len(X_train.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sentence, vector) in enumerate(zip(X_train, X_train_vect_avg)):\n",
    "    print(\"Original Sentence Length:\", len(sentence))\n",
    "    print(\"Averaged Sentence Vector Length:\", len(vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f940055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressor = LogisticRegression(max_iter=400)                                                 \n",
    "regressor = regressor.fit(X_train_vect_avg, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = regressor.predict(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['breakfast','cereal','dinner','lunch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402ba9c",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285015d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('glove.txt', binary=False,no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d364e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vector('light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e982627",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(model.index_to_key )\n",
    "#For each sequence of words in X_train, the list comprehension [model[i] for i in ls if i in words] iterates through each word (i) in the sequence ls and retrieves its word vector using model[i].\n",
    "X_train_vect = pad_sequences([[model[i] for i in ls if i in words] for ls in X_train], dtype='float32', padding='post')\n",
    "X_test_vect = pad_sequences([[model[i] for i in ls if i in words] for ls in X_test], dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2135a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressor = LogisticRegression(max_iter=10000)                               \n",
    "regressor = regressor.fit(X_train_vect_avg, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = regressor.predict(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2744021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece6f76",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Load into gensim\n",
    "w2vec = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43632e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2vec.index_to_key )\n",
    "#For each sequence of words in X_train, the list comprehension [model[i] for i in ls if i in words] iterates through each word (i) in the sequence ls and retrieves its word vector using model[i].\n",
    "X_train_vect = pad_sequences([[w2vec[i] for i in ls if i in words] for ls in X_train], dtype='float32', padding='post')\n",
    "X_test_vect = pad_sequences([[w2vec[i] for i in ls if i in words] for ls in X_test], dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90221245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressor = LogisticRegression(max_iter=10000)                               \n",
    "regressor = regressor.fit(X_train_vect_avg, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = regressor.predict(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c07bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a299cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
