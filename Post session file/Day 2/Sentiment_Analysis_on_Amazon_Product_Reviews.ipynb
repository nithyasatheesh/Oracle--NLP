{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KSnwuemmnDr7",
   "metadata": {
    "id": "KSnwuemmnDr7"
   },
   "source": [
    "ðŸ“Œ In this notebook, our main goal is to categorize product reviews into two distinct categories: positive and negative. To achieve this goal, we use various techniques of natural language processing and machine learning to analyze and classify the emotions expressed in the notebook accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9nSc8gHQsI1",
   "metadata": {
    "id": "b9nSc8gHQsI1"
   },
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rYYZ8XhfBhty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYYZ8XhfBhty",
    "outputId": "6821e3f3-59db-40cc-c209-6550b6fd116f"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "\n",
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "#filterwarnings(\"ignore\")\n",
    "#pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.width\", 500)\n",
    "#pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhciD8n-ROWT",
   "metadata": {
    "id": "BhciD8n-ROWT"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mey4UsoMO-Mq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "Mey4UsoMO-Mq",
    "outputId": "c3dac881-c53e-4f09-8f47-6a1616f51398"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D://Files//amazon_review.csv\", sep=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dgn7P3sySvNg",
   "metadata": {
    "id": "dgn7P3sySvNg"
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PFchKF_LcyG0",
   "metadata": {
    "id": "PFchKF_LcyG0"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(dataframe, dependent_var):\n",
    "  # Normalizing Case Folding - Uppercase to Lowercase\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "  # Removing Punctuation\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]','')\n",
    "\n",
    "  # Removing Numbers\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d','')\n",
    "\n",
    "  # StopWords\n",
    "  sw = stopwords.words('english')\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "  # Remove Rare Words\n",
    "  temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "  drops = temp_df[temp_df <= 1]\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "  # Lemmatize\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wQhVphbxeP2e",
   "metadata": {
    "id": "wQhVphbxeP2e"
   },
   "outputs": [],
   "source": [
    "df = text_preprocessing(df, \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rujcewXWeeXc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rujcewXWeeXc",
    "outputId": "779fc9db-d926-4beb-aa7f-702329514f38"
   },
   "outputs": [],
   "source": [
    "df[\"reviewText\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q5GgvSpL_N2c",
   "metadata": {
    "id": "Q5GgvSpL_N2c"
   },
   "source": [
    "# Text Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2mOn-AmtIPvA",
   "metadata": {
    "id": "2mOn-AmtIPvA"
   },
   "outputs": [],
   "source": [
    "def text_visulaization(dataframe, dependent_var, barplot=True, wordcloud=True):\n",
    "  # Calculation of Term Frequencies\n",
    "  tf = dataframe[dependent_var].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
    "  tf.columns = [\"words\", \"tf\"]\n",
    "\n",
    "  if barplot:\n",
    "    # Bar Plot\n",
    "    tf[tf[\"tf\"]>1000].plot.barh(x=\"words\", y=\"tf\")\n",
    "    plt.title(\"Calculation of Term Frequencies : barplot\")\n",
    "    plt.show()\n",
    "\n",
    "  if wordcloud:\n",
    "    # WordCloud\n",
    "    text = \" \".join(i for i in dataframe[dependent_var])\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=1000, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Calculation of Term Frequencies : wordcloud\")\n",
    "    plt.show()\n",
    "    wordcloud.to_file(\"wordcloud.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3QhiC49sL1O2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "3QhiC49sL1O2",
    "outputId": "daaa4520-f134-4ba3-9fba-36cd25039740"
   },
   "outputs": [],
   "source": [
    "text_visulaization(df, \"reviewText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vICvqSqoRVjB",
   "metadata": {
    "id": "vICvqSqoRVjB"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcJD2t46S2QJ",
   "metadata": {
    "id": "FcJD2t46S2QJ"
   },
   "outputs": [],
   "source": [
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AkPXhZIJTTbS",
   "metadata": {
    "id": "AkPXhZIJTTbS"
   },
   "outputs": [],
   "source": [
    "create_polarity_scores(df, \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30lKexlqTXCf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "30lKexlqTXCf",
    "outputId": "1e70b6e6-158b-4bfc-ef50-0dbb0f210c81"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z3RNYQpZcz6z",
   "metadata": {
    "id": "Z3RNYQpZcz6z"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hca9hvo_-Noi",
   "metadata": {
    "id": "hca9hvo_-Noi"
   },
   "outputs": [],
   "source": [
    "# Create Lables\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "  dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "  X = dataframe[dependent_var]\n",
    "  y = dataframe[independent_var]\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_y9FTFBNFF0",
   "metadata": {
    "id": "1_y9FTFBNFF0"
   },
   "outputs": [],
   "source": [
    "X, y = create_label(df, \"reviewText\", \"sentiment_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BU41UujB9Fej",
   "metadata": {
    "id": "BU41UujB9Fej"
   },
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "def split_dataset(dataframe, X, y):\n",
    "  train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "  return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GsKr4K2aNOlf",
   "metadata": {
    "id": "GsKr4K2aNOlf"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cP8sxBWNuUB",
   "metadata": {
    "id": "2cP8sxBWNuUB"
   },
   "outputs": [],
   "source": [
    "def create_features_count(train_x, test_x):\n",
    "  # Count Vectors\n",
    "  vectorizer = CountVectorizer()\n",
    "  x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "  x_test_count_vectorizer = vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_count_vectorizer, x_test_count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vWZ7jvmdOAPb",
   "metadata": {
    "id": "vWZ7jvmdOAPb"
   },
   "outputs": [],
   "source": [
    "x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OACN2blHOKjq",
   "metadata": {
    "id": "OACN2blHOKjq"
   },
   "outputs": [],
   "source": [
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "  # TF-IDF word\n",
    "  tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "  x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "  x_test_tf_idf_word = tf_idf_word_vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_tf_idf_word, x_test_tf_idf_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GssiWOCnOUgB",
   "metadata": {
    "id": "GssiWOCnOUgB"
   },
   "outputs": [],
   "source": [
    "x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sKuR-Ks7OZEB",
   "metadata": {
    "id": "sKuR-Ks7OZEB"
   },
   "outputs": [],
   "source": [
    "#optional\n",
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "  # TF-IDF ngram\n",
    "  tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "  x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "  x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_tf_idf_ngram, x_test_tf_idf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99_xVvyCOgEO",
   "metadata": {
    "id": "99_xVvyCOgEO"
   },
   "outputs": [],
   "source": [
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrM1bIQtOvqL",
   "metadata": {
    "id": "wrM1bIQtOvqL"
   },
   "source": [
    "# Sentiment Modeling - Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nwCKKdllOqeK",
   "metadata": {
    "id": "nwCKKdllOqeK"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def crate_model_logistic(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  loj_count = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_count = loj_count.fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(loj_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "  # TF-IDF Word\n",
    "  x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "  loj_word = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_word = loj_word.fit(x_train_tf_idf_word, train_y)\n",
    "  accuracy_word = cross_val_score(loj_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "  # TF-IDF ngram\n",
    "  x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "  loj_ngram = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_ngram = loj_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "  accuracy_ngram = cross_val_score(loj_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    " \n",
    "  return loj_model_count, loj_model_word, loj_model_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OQhLgG0JPg9m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQhLgG0JPg9m",
    "outputId": "bdfa5d46-acc9-495c-dacf-3060a888b6a9"
   },
   "outputs": [],
   "source": [
    "loj_model_count, loj_model_word, loj_model_ngram= crate_model_logistic(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MwWkxDWrQ9O6",
   "metadata": {
    "id": "MwWkxDWrQ9O6"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def crate_model_randomforest(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  rf_count = RandomForestClassifier()\n",
    "  rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "  # TF-IDF Word\n",
    "  x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "  rf_word = RandomForestClassifier()\n",
    "  rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "  accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "  # TF-IDF ngram\n",
    "  x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "  rf_ngram = RandomForestClassifier()\n",
    "  rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "  accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    " \n",
    "  return rf_model_count, rf_model_word, rf_model_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2hDpJWePTl8p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hDpJWePTl8p",
    "outputId": "7d9e4820-c989-43ba-bffc-f82d859d2eea"
   },
   "outputs": [],
   "source": [
    "rf_model_count, rf_model_word, rf_model_ngram = crate_model_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cfpi5_dHUUvt",
   "metadata": {
    "id": "Cfpi5_dHUUvt"
   },
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MK0sppO6TvgI",
   "metadata": {
    "id": "MK0sppO6TvgI"
   },
   "outputs": [],
   "source": [
    "def model_tuning_randomforest(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  rf_model_count = RandomForestClassifier(random_state=1)\n",
    "  rf_params = {\"max_depth\": [2,5,8, None],\n",
    "               \"max_features\": [2,5,8, \"auto\"],\n",
    "               \"n_estimators\": [100,500,1000],\n",
    "               \"min_samples_split\": [2,5,10]}\n",
    "  rf_best_grid = GridSearchCV(rf_model_count, rf_params, cv=10, n_jobs=-1, verbose=False).fit(x_train_count_vectorizer, train_y)\n",
    "  rf_model_count_final = rf_model_count.set_params(**rf_best_grid.best_params_, random_state=1).fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(rf_model_count_final, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "  return rf_model_count_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kCFDqcvYVI0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kCFDqcvYVI0",
    "outputId": "cb550bb1-ed0e-49e0-b005-f0a8739764f8"
   },
   "outputs": [],
   "source": [
    "rf_model_count_final = model_tuning_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x9CSxGnAhEa1",
   "metadata": {
    "id": "x9CSxGnAhEa1"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WMPUuVNyYfiI",
   "metadata": {
    "id": "WMPUuVNyYfiI"
   },
   "outputs": [],
   "source": [
    "def predict_count(train_x, model, new_comment):\n",
    "  new_comment= pd.Series(new_comment)\n",
    "  new_comment = CountVectorizer().fit(train_x).transform(new_comment)\n",
    "  result = model.predict(new_comment)\n",
    "  if result==1:\n",
    "    print(\"Comment is Positive\")\n",
    "  else:\n",
    "    print(\"Comment is Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--Q5Pp5Hk-gO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--Q5Pp5Hk-gO",
    "outputId": "23ec1ec9-af42-481a-d5a4-83e64a32be11"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "predict_count(train_x, model=loj_model_count, new_comment=\"this product is very good :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TaHF_r8FlGmc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaHF_r8FlGmc",
    "outputId": "620aa2f7-4e20-46e8-fe55-d7bc3df87d09"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=\"this product is very bad :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dDcRbmW-mz9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDcRbmW-mz9N",
    "outputId": "60a84280-5b0c-4a38-bbf8-633e67efa11b"
   },
   "outputs": [],
   "source": [
    "# Sample Review\n",
    "new_comment=pd.Series(df[\"reviewText\"].sample(1).values)\n",
    "new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oxPBlbkjmOYW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxPBlbkjmOYW",
    "outputId": "cc152799-9435-4dcf-ce16-df5f634bf4ea"
   },
   "outputs": [],
   "source": [
    "# Sample Review - Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=new_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56d20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
